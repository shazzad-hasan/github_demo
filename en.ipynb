{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "en.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNJUnvvFYDIHhzHg+ZFxwk1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazzad-hasan/github_demo/blob/main/en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyq56XZKeZvj",
        "outputId": "f379efb1-61b9-4db1-e0fb-ff6268ebc5ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Questions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNNY0r4qeZym",
        "outputId": "df112979-6838-438d-ff2a-5bc42b8a9de3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Questions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrmCq4QGeZ1b",
        "outputId": "a6a31abd-b479-4cb6-8961-3cee91b2741b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus.train.bn  corpus.train.en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('corpus.train.en', encoding = 'utf8').read()"
      ],
      "metadata": {
        "id": "-LWC6hvcXjWe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "PuaAfCfIhD3y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, re, pprint, string\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
        "string.punctuation = string.punctuation.replace('.', '')"
      ],
      "metadata": {
        "id": "qytZHZmGXRcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9a8e46-f782-4fd8-d822-34bb3e5f6837"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess data\n",
        "file_nl_removed = \"\"\n",
        "for line in file:\n",
        "  line_nl_removed = line.replace(\"\\n\", \" \")      #removes newlines\n",
        "  file_nl_removed += line_nl_removed\n",
        "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters"
      ],
      "metadata": {
        "id": "I48KW38AXRfu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents = nltk.sent_tokenize(file_p)\n",
        "print(\"The number of sentences is\", len(sents)) \n",
        "#prints the number of sentences\n",
        "words = nltk.word_tokenize(file_p)\n",
        "print(\"The number of tokens is\", len(words)) \n",
        "#prints the number of tokens\n",
        "average_tokens = round(len(words)/len(sents))\n",
        "print(\"The average number of tokens per sentence is\",\n",
        "average_tokens) \n",
        "#prints the average number of tokens per sentence\n",
        "unique_tokens = set(words)\n",
        "print(\"The number of unique tokens are\", len(unique_tokens)) \n",
        "#prints the number of unique tokens"
      ],
      "metadata": {
        "id": "5u9GgbXpXRi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fb7df0-431d-49c0-98c9-43c2cf6be19f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of sentences is 1722902\n",
            "The number of tokens is 32637428\n",
            "The average number of tokens per sentence is 19\n",
            "The number of unique tokens are 508469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "bigram=[]\n",
        "\n",
        "tokenized_text = []\n",
        "for sentence in sents:\n",
        "    sentence = sentence.lower()\n",
        "    sequence = word_tokenize(sentence) \n",
        "    tokenized_text.append(sequence) \n",
        "    bigram.extend(list(ngrams(sequence, 2)))  \n"
      ],
      "metadata": {
        "id": "VUvszDM5XRlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b71394e0-c641-43b3-c63e-f7e81890167c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def removal(x):     \n",
        "#removes ngrams containing only stopwords\n",
        "    y = []\n",
        "    for pair in x:\n",
        "        count = 0\n",
        "        for word in pair:\n",
        "            if word in stop_words:\n",
        "                count = count or 0\n",
        "            else:\n",
        "                count = count or 1\n",
        "        if (count==1):\n",
        "            y.append(pair)\n",
        "    return(y)\n",
        "bigram = removal(bigram)\n",
        "\n",
        "freq_bi = nltk.FreqDist(bigram)\n",
        "\n",
        "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
        "print (\"Most common bigrams: \", freq_bi.most_common(5))"
      ],
      "metadata": {
        "id": "GyKqAosvDgBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Calculate the frequency of the words inside\n",
        "frequency = {}\n",
        "for word in sents:\n",
        "    count = frequency.get(word , 0)\n",
        "    frequency[ word ] = count + 1"
      ],
      "metadata": {
        "id": "0gc46rBwQUm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank = 1\n",
        "column_header = ['Rank', 'Frequency', 'Frequency * Rank']\n",
        "df = pd.DataFrame( columns = column_header )\n",
        "collection = sorted(frequency.items(), key=itemgetter(1), reverse = True)"
      ],
      "metadata": {
        "id": "WHJ9TXSshl5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word , freq in collection[:10]:\n",
        "    df.loc[word] = [rank, freq, rank*freq]\n",
        "    rank = rank + 1\n",
        "    \n",
        "print (df)"
      ],
      "metadata": {
        "id": "2HRmNdgin4xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Python visualization with pyplot\n",
        "plt.figure(figsize=(20,20))  #to increase the plot resolution\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.xticks(rotation=90)    #to rotate x-axis values\n",
        "\n",
        "for word , freq in collection[:30]:\n",
        "    plt.bar(word, freq)    \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O-18V8ZlhmAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#Stemming the words\n",
        "#steming_words = [stemmer.stem(word) for word in words if not word in set(stopwords.words('english'))]\n",
        "\n",
        "for word in words:\n",
        "    print(word+' -> '+ stemmer.stem(word))"
      ],
      "metadata": {
        "id": "1pBJUVJmhmDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#Lemmatizing the words\n",
        "lematizing_words = [lemmatizer.lemmatize(word) for word in words if not word in set(stopwords.words('english'))]\n",
        "\n",
        "#for word in words:\n",
        "#    print(word+' -> '+ lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "id": "gpBbp8mIrSQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = []\n",
        "cleaned = []\n",
        "for s in sents:\n",
        "    if s in cleaned:\n",
        "        if s in duplicates:\n",
        "            continue\n",
        "        else:\n",
        "            duplicates.append(s)\n",
        "            print(duplicates)\n",
        "    else:\n",
        "        cleaned.append(s)"
      ],
      "metadata": {
        "id": "RxYyaojBsCOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer()\n",
        "\n",
        "tf_idf =  vec.fit_transform(sents)\n",
        "print(pd.DataFrame(tf_idf.toarray(), columns=vec.get_feature_names()))"
      ],
      "metadata": {
        "id": "bn_wzfwpAaeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer()\n",
        "\n",
        "tf_idf =  vec.fit_transform(sents[:5])\n",
        "print(pd.DataFrame(tf_idf.toarray(), columns=vec.get_feature_names()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAOsG5tOBl-d",
        "outputId": "1f96282b-982e-4209-9fa9-2c6bdac63afb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       also       and        as        be  better     bores  clemenceau  \\\n",
            "0  0.000000  0.000000  0.000000  0.000000     1.0  0.000000    0.000000   \n",
            "1  0.000000  0.000000  0.258199  0.258199     0.0  0.000000    0.000000   \n",
            "2  0.000000  0.226831  0.000000  0.000000     0.0  0.000000    0.000000   \n",
            "3  0.000000  0.000000  0.000000  0.000000     0.0  0.000000    0.000000   \n",
            "4  0.175006  0.141194  0.000000  0.000000     0.0  0.175006    0.350011   \n",
            "\n",
            "   commotions   company  complained  ...      they        to     trump  \\\n",
            "0    0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
            "1    0.000000  0.000000    0.000000  ...  0.000000  0.258199  0.000000   \n",
            "2    0.000000  0.000000    0.000000  ...  0.281151  0.000000  0.281151   \n",
            "3    0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
            "4    0.175006  0.175006    0.175006  ...  0.000000  0.000000  0.000000   \n",
            "\n",
            "       warm      will    wilson   wilsons    wishes      with       you  \n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1  0.000000  0.258199  0.000000  0.000000  0.258199  0.000000  0.000000  \n",
            "2  0.281151  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.707107  \n",
            "4  0.000000  0.000000  0.175006  0.175006  0.000000  0.350011  0.000000  \n",
            "\n",
            "[5 rows x 48 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents[1].lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "65zvkCWgS5Cw",
        "outputId": "6736df35-3eac-4da7-921e-05b09cf8f245"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'guaranteeing that every individual will be free to do as he wishes inevitably shortchanges equality.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hI-83EIxS6Xi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}