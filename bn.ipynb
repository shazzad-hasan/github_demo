{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bn.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNGmAPA3VbjEkVRc2EofqJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazzad-hasan/github_demo/blob/main/bn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKKiqPew0lvP",
        "outputId": "d4345285-55ae-4f52-b4ec-c409b36ac561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Questions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhQS08wE0tI1",
        "outputId": "976e4e00-7a1b-487c-e30d-bf659e37f954"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Questions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEblaE_h09Ba",
        "outputId": "19168860-896c-4a72-eaed-aaf114982d69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus.train.bn  corpus.train.en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('corpus.train.bn', encoding = 'utf8').read()"
      ],
      "metadata": {
        "id": "ltfsa3iq09FH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bnlp_toolkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCBM88cd62yQ",
        "outputId": "24db1068-4d30-457e-e0bf-688a010c081a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bnlp_toolkit\n",
            "  Downloading bnlp_toolkit-3.1.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from bnlp_toolkit) (1.4.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from bnlp_toolkit) (3.2.5)\n",
            "Collecting gensim==4.0.1\n",
            "  Downloading gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9 MB 840 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bnlp_toolkit) (1.21.6)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.7/dist-packages (from bnlp_toolkit) (0.9.1)\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.1->bnlp_toolkit) (6.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->bnlp_toolkit) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (4.64.0)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 65.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (0.8.9)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, sentencepiece, gensim, bnlp-toolkit\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed bnlp-toolkit-3.1.2 gensim-4.0.1 python-crfsuite-0.9.8 sentencepiece-0.1.96 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIcAYBYtJotq",
        "outputId": "9608d794-1566-4574-ec1d-a0702acbb7c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bltk\n",
            "  Downloading bltk-1.2.tar.gz (17.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.4 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn>=0.0 in /usr/local/lib/python3.7/dist-packages (from bltk) (0.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from bltk) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from bltk) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from bltk) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from bltk) (1.21.6)\n",
            "Collecting nltk>=3.4.5\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.7/dist-packages (from bltk) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2019.11.28 in /usr/local/lib/python3.7/dist-packages (from bltk) (2021.10.8)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2022.4.24-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4.5->bltk) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4.5->bltk) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->bltk) (3.1.0)\n",
            "Building wheels for collected packages: bltk\n",
            "  Building wheel for bltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bltk: filename=bltk-1.2-py3-none-any.whl size=17432539 sha256=bf36e5a6283cd8e35cd5eabf35a2db1be085b4ba5d95bccb7cbed0c93eda786b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/4f/91/e074e661b4dcbc24a83e050d1c75cecfa186ffe9d58b641c51\n",
            "Successfully built bltk\n",
            "Installing collected packages: regex, nltk, bltk\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed bltk-1.2 nltk-3.7 regex-2022.4.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.util import ngrams\n",
        "\n",
        "import bnlp\n",
        "from bnlp import NLTKTokenizer\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjX0sbyv09IJ",
        "outputId": "757083dd-063e-4aef-b0c8-01b20a51d99b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "punkt not found. downloading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# preprocess dataset \n",
        "\n",
        "file_nl_removed = \"\"\n",
        "\n",
        "for line in file:\n",
        "  # remove new lines\n",
        "  line_nl_removed = line.replace(\"\\n\", \" \")     \n",
        "  file_nl_removed += line_nl_removed\n",
        "  \n",
        "from bltk.langtools.banglachars import (vowels,\n",
        "                                        vowel_signs,\n",
        "                                        consonants,\n",
        "                                        digits,\n",
        "                                        operators,\n",
        "                                        punctuations,\n",
        "                                        others)\n",
        "# remove all special characters\n",
        "file_p = \"\".join([char for char in file_nl_removed])"
      ],
      "metadata": {
        "id": "qSwdlkT_Fuwx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(file_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yxFXszPL0Uy",
        "outputId": "d3bdd6d9-bf74-4f18-f18a-51c01b9d20cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_p[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "R8_RhCWKL0X-",
        "outputId": "da31be94-07e4-4811-a856-66470ad7b9fb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'আবার সবাইকে যার যার ইচ্ছামতো চলতে দিলে সমতা রক্ষা করা অসম্ভব হয়ে যায়। তুরুপ পদদলিত করা \"সেগুলো হৃদয়কে উষ্ণ করে এবং রোজকার বোঝাগুলোকে হালকা করে। আমি ভালোবাসি তোমাকে । পোর্ট কোম্পানি লিমিটেড - কেপিসিএল তোলপাড় এছাড়াও ক্লেমঁসো উইলসনের ১৪ দফার ব্যাপারে সংশয়ী এবং হতাশ ছিলেন। তিনি অভিযোগ করে বলেন, \"মিস্টার উইলসনের ১৪ দফা বিরক্তিকর। আল-জাজিরার সঙ্গে জালুদের সাক্ষাতকার এবং তার দৃষ্টিভঙ্গীর বিষয়ে অন্যান্য টুইটার ব্যবহারকারীরা মন্তব্য করেন: ক্লাব. তাই আসুন এখন আমরা একবার পরীক্ষা করে দেখি যে তাদের এই দাবি সত্যি কি না। ওমানের শাসক সুলতান কাবুস বিন সাইদের সমালোচনা করার অপরাধে আলরাওয়াহিকে শাস্তি প্রদানের পর গ্লোবাল ভয়েসেস এডভোকেসিতে ২০১২ সালে তাকে লক্ষ্যনীয়ভাবে উপস্থাপন করা হয়। আমাকে না বললেও আমি কাউকে বলতাম না। ভালুককে একটা আলিঙ্গন করে জড়িয়ে ধরে। \" (১ পিতর ৫:৫) আমরা আমাদের ভাইদের সঙ্গে যেভাবে ব্যবহার করি তা যিহোবার সঙ্গে আমাদের সম্পর্কের ওপর একটা বড় ছাপ ফেলে।- ১ যোহন ৪:২০. দায়িত্ব. উনি ফল ছাড়া আর কিছুই খান না। আমার নাম এজাক্স! এর সদস্যদের মধ্যে একজন ছিলেন লন্ডনস্থ ইন্ডিয়া হাউসের বীরেন চট্টোপাধ্যায়. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_p = \"\".join(i for i in file_p[:1000] if i in [\"।\"] or 2432 <= ord(i) <= 2559 or ord(i)== 32)\n"
      ],
      "metadata": {
        "id": "50WJej6vL0bH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dqYv-ipOUnTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 7: Data Statistics"
      ],
      "metadata": {
        "id": "DESJn7S31MeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnltk = NLTKTokenizer()\n",
        "\n",
        "sentences = bnltk.sentence_tokenize(file_p)\n",
        "print(\"The number of sentences is\", len(sentences)) \n",
        "\n",
        "words = bnltk.word_tokenize(file_p)\n",
        "print(\"The number of tokens is\", len(words)) \n",
        "\n",
        "average_tokens = round(len(words)/len(sentences))\n",
        "print(\"The average number of tokens per sentence is\", average_tokens) \n",
        "\n",
        "unique_tokens = set(words)\n",
        "print(\"The number of unique tokens are\", len(unique_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jyOnleU09Q4",
        "outputId": "9a5bf5a4-22f3-4f46-c763-037ce04e66c4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of sentences is 12\n",
            "The number of tokens is 172\n",
            "The average number of tokens per sentence is 14\n",
            "The number of unique tokens are 140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: N-gram Models (bigram, trigram)\n"
      ],
      "metadata": {
        "id": "VyKiA0Yf1SWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp.corpus import stopwords, punctuations, letters, digits\n",
        "from bnlp.corpus.util import remove_stopwords\n",
        "\n",
        "\n",
        "stop_words = remove_stopwords(sentences, stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "zidRp1rM09T3",
        "outputId": "604283c9-4497-4e15-c3c6-86c1534f3936"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-818c5b7844b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bnlp/corpus/util.py\u001b[0m in \u001b[0;36mremove_stopwords\u001b[0;34m(text, stopwords)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \"\"\"\n\u001b[1;32m     13\u001b[0m   \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mfiltered_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bnlp/tokenizer/basic.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m\"\"\"Tokenizes a piece of text.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bnlp/tokenizer/basic.py\u001b[0m in \u001b[0;36mconvert_to_unicode\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsupported string type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unsupported string type: <class 'list'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bigram[:5])"
      ],
      "metadata": {
        "id": "CHFbtQfO09Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trigram[:5])"
      ],
      "metadata": {
        "id": "exr_c_tx09Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Word frequencies using Zipf's law"
      ],
      "metadata": {
        "id": "An6WpMdV1cc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "# Calculate the frequency of the words inside\n",
        "frequency = {}\n",
        "for word in words[:50]:\n",
        "    count = frequency.get(word , 0)\n",
        "    frequency[ word ] = count + 1\n",
        "\n",
        "rank = 1\n",
        "column_header = ['Rank', 'Frequency', 'Frequency * Rank']\n",
        "df = pd.DataFrame( columns = column_header )\n",
        "collection = sorted(frequency.items(), key=itemgetter(1), reverse = True)\n",
        "\n",
        "for word , freq in collection[:10]:\n",
        "    df.loc[word] = [rank, freq, rank*freq]\n",
        "    rank = rank + 1\n",
        "    \n",
        "print(df)"
      ],
      "metadata": {
        "id": "LT80op3Q09gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python visualization with pyplot\n",
        "plt.figure(figsize=(20,20)) \n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "for word , freq in collection[:30]:\n",
        "    plt.bar(word, freq)    \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mlTpUuuy09jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4: Stemming and Lemmatisation"
      ],
      "metadata": {
        "id": "jIrbvq8s1if_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stemming the words\n",
        "steming_words = [stemmer.stem(word) for word in words[:50] if not word in set(stopwords.words('english'))]\n",
        "\n",
        "for word in words[:20]:\n",
        "  if not word in set(stopwords.words('english')):\n",
        "    print(word+' -> '+ stemmer.stem(word))"
      ],
      "metadata": {
        "id": "egvtGhTb09mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatizing the words\n",
        "lematizing_words = [lemmatizer.lemmatize(word) for word in words[:50] if not word in set(stopwords.words('english'))]\n",
        "\n",
        "for word in words[:20]:\n",
        "  if not word in set(stopwords.words('english')):\n",
        "    print(word+' -> '+ lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "id": "C3-uX00J09pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5: Sentence repetition"
      ],
      "metadata": {
        "id": "VihGuH8k1tcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = []\n",
        "cleaned = []\n",
        "for sentence in sentences[:2000]:\n",
        "    if sentence in cleaned:\n",
        "        if sentence in duplicates:\n",
        "            continue\n",
        "        else:\n",
        "            duplicates.append(sentence)\n",
        "    else:\n",
        "        cleaned.append(sentence)\n",
        "\n",
        "print(duplicates[:2])"
      ],
      "metadata": {
        "id": "o1kHUJD909sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6: Tf-Idf measurements"
      ],
      "metadata": {
        "id": "Fz3cBuzb1xsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer()\n",
        "\n",
        "tf_idf =  vec.fit_transform(sentences[:10])\n",
        "print(pd.DataFrame(tf_idf.toarray(), columns=vec.get_feature_names()))"
      ],
      "metadata": {
        "id": "b1qR8w3M09vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9UHKtSIk09yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4KfvciaL091V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}