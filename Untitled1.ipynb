{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOOrcNh+v6JRUidvGBPpmHG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazzad-hasan/github_demo/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyq56XZKeZvj",
        "outputId": "bf20726d-4010-4af9-8878-5704994b7da2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Questions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNNY0r4qeZym",
        "outputId": "4eadac08-23da-4028-e77d-2d4015014bee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Questions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrmCq4QGeZ1b",
        "outputId": "cdaeeb46-296c-4184-eca8-4c4263c9bb3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus.train.bn  corpus.train.en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('corpus.train.en', encoding = 'utf8').read()"
      ],
      "metadata": {
        "id": "-LWC6hvcXjWe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, re, pprint, string\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
        "string.punctuation = string.punctuation.replace('.', '')"
      ],
      "metadata": {
        "id": "qytZHZmGXRcf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess data\n",
        "file_nl_removed = \"\"\n",
        "for line in file:\n",
        "  line_nl_removed = line.replace(\"\\n\", \" \")      #removes newlines\n",
        "  file_nl_removed += line_nl_removed\n",
        "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "I48KW38AXRfu",
        "outputId": "7c231278-c6ad-4204-afc0-5fa3c6ec13e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-cc2a64140b86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mline_nl_removed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#removes newlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mfile_nl_removed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mline_nl_removed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfile_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_nl_removed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#removes all special characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents = nltk.sent_tokenize(file_p)\n",
        "print(\"The number of sentences is\", len(sents)) \n",
        "#prints the number of sentences\n",
        "words = nltk.word_tokenize(file_p)\n",
        "print(\"The number of tokens is\", len(words)) \n",
        "#prints the number of tokens\n",
        "average_tokens = round(len(words)/len(sents))\n",
        "print(\"The average number of tokens per sentence is\",\n",
        "average_tokens) \n",
        "#prints the average number of tokens per sentence\n",
        "unique_tokens = set(words)\n",
        "print(\"The number of unique tokens are\", len(unique_tokens)) \n",
        "#prints the number of unique tokens"
      ],
      "metadata": {
        "id": "5u9GgbXpXRi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "bigram=[]\n",
        "\n",
        "tokenized_text = []\n",
        "for sentence in sents:\n",
        "    sentence = sentence.lower()\n",
        "    sequence = word_tokenize(sentence) \n",
        "    tokenized_text.append(sequence) \n",
        "    bigram.extend(list(ngrams(sequence, 2)))  \n",
        "\n",
        "def removal(x):     \n",
        "#removes ngrams containing only stopwords\n",
        "    y = []\n",
        "    for pair in x:\n",
        "        count = 0\n",
        "        for word in pair:\n",
        "            if word in stop_words:\n",
        "                count = count or 0\n",
        "            else:\n",
        "                count = count or 1\n",
        "        if (count==1):\n",
        "            y.append(pair)\n",
        "    return(y)\n",
        "bigram = removal(bigram)\n",
        "\n",
        "freq_bi = nltk.FreqDist(bigram)\n",
        "\n",
        "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
        "print (\"Most common bigrams: \", freq_bi.most_common(5))\n"
      ],
      "metadata": {
        "id": "VUvszDM5XRlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Import libraries required\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "PuaAfCfIhD3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Calculate the frequency of the words inside\n",
        "frequency = {}\n",
        "for word in sents:\n",
        "    count = frequency.get(word , 0)\n",
        "    frequency[ word ] = count + 1"
      ],
      "metadata": {
        "id": "0gc46rBwQUm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank = 1\n",
        "column_header = ['Rank', 'Frequency', 'Frequency * Rank']\n",
        "df = pd.DataFrame( columns = column_header )\n",
        "collection = sorted(frequency.items(), key=itemgetter(1), reverse = True)"
      ],
      "metadata": {
        "id": "WHJ9TXSshl5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word , freq in collection[:10]:\n",
        "    df.loc[word] = [rank, freq, rank*freq]\n",
        "    rank = rank + 1\n",
        "    \n",
        "print (df)"
      ],
      "metadata": {
        "id": "2HRmNdgin4xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Python visualization with pyplot\n",
        "plt.figure(figsize=(20,20))  #to increase the plot resolution\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.xticks(rotation=90)    #to rotate x-axis values\n",
        "\n",
        "for word , freq in collection[:30]:\n",
        "    plt.bar(word, freq)    \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O-18V8ZlhmAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#Stemming the words\n",
        "#steming_words = [stemmer.stem(word) for word in words if not word in set(stopwords.words('english'))]\n",
        "\n",
        "for word in words:\n",
        "    print(word+' -> '+ stemmer.stem(word))"
      ],
      "metadata": {
        "id": "1pBJUVJmhmDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#Lemmatizing the words\n",
        "lematizing_words = [lemmatizer.lemmatize(word) for word in words if not word in set(stopwords.words('english'))]\n",
        "\n",
        "#for word in words:\n",
        "#    print(word+' -> '+ lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "id": "gpBbp8mIrSQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = []\n",
        "cleaned = []\n",
        "for s in sents:\n",
        "    if s in cleaned:\n",
        "        if s in duplicates:\n",
        "            continue\n",
        "        else:\n",
        "            duplicates.append(s)\n",
        "            print(duplicates)\n",
        "    else:\n",
        "        cleaned.append(s)"
      ],
      "metadata": {
        "id": "RxYyaojBsCOP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}